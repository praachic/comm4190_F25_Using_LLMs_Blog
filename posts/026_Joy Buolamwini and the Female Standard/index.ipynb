{
 "cells": [
  {
   "cell_type": "raw",
   "id": "fe7a7e49-233e-4e45-856d-b7a637d615e7",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Joy Buolamwini and the Female Standard\"\n",
    "description: \"How do system reinforce unfair standards?\"\n",
    "author: \"Praachi Chakraborty\"\n",
    "date: \"12/15/25\"\n",
    "categories:\n",
    "  - Unmasking AI\n",
    "  - AI Biases\n",
    "  - Joy Buolamwini\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6babcfa-5e2d-48b1-a71c-d91845793138",
   "metadata": {},
   "source": [
    "### Setup:\n",
    "\n",
    "While the primary focus of Unmasking AI has been on identifying racial biases in facial recognition systems, Chapter 13 of the book, titled AI Ain’t AI, Women, shows how these same biases can extend into other categories, such as gender. In this section, Joy Buolamwini tests multiple facial recognition systems on identifying gender.\n",
    "\n",
    "To do so, Buolamwini tested two facial recognition systems from Amazon and IBM on images of well-known Black women. She particularly chose famous figures to eliminate the excuse of having poor data, and from a human standpoint, it is clear that these two people are highly feminine. Buolamwini’s results show that Oprah Winfrey received the label of, as you can see in the image below, “appears to be male” with an accuracy or certainty of 76.5%, and Sojourner Truth received a title from IBM Watson of “clean-shaven adult male” with a certainty of 77%. Notably, both of these figures display clearly feminine qualities that one would expect AI may pick up on. For example, Oprah Winfrey’s image shows her wearing makeup and jewelry.\n",
    "\n",
    "<img src=\"Oprah.png\" width=\"50%\"/>\n",
    "\n",
    "<img src=\"Sojourner.png\" width=\"50%\"/>\n",
    "\n",
    "\n",
    "These results emphasize the importance and ripple effect of training data on AI’s analytical ability. As we know, what the facial recognition systems are doing is trying to identify patterns in the data based on the training data it was provided, which likely contained an overabundance of white male faces, such that it found it difficult to identify people of different races and ethnicities. The accuracy rate of male subjects across the board was significantly higher than that of female subjects, with some models having as high as a 20% difference in error rate.\n",
    "\n",
    "Effects are applied onto the exploitation of AI, such as through police surveillance or hiring practices. It creates real-world inequalities that have the potential for great harm. The issue here is not just the performance of these two facial recognition models. Fixing these errors is a step in the right direction, but the issue lies in how models like this are trained across the board, and how the inequalities in training data can ripple into other practices. What I hear is not creating new concepts, but rather mirroring and amplifying the data that it has been given by society."
   ]
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
