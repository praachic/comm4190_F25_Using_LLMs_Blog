{
 "cells": [
  {
   "cell_type": "raw",
   "id": "fe7a7e49-233e-4e45-856d-b7a637d615e7",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Can ChatGPT Start Without a Plan?\"\n",
    "description: \"LLMs Unguided\"\n",
    "author: \"Praachi Chakraborty\"\n",
    "date: \"12/10/25\"\n",
    "categories:\n",
    "  - Theoretical vs Practical\n",
    "  - Training AI\n",
    "  - ChatGPT\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6babcfa-5e2d-48b1-a71c-d91845793138",
   "metadata": {},
   "source": [
    "### Setup:\n",
    "\n",
    "When we use LLMs, we tend to find ways to use them for our benefit. We construct elaborate plans and prompt them to perfection. The other day, I stumbled across an Instagram reel that was talking about something called a “Yes Day” that some parents do. It runs on a similar idea: for most of a child’s early life, they are told to do certain things and told the ways they’re supposed to do them. Even if they have agency to make choices and decisions, they don’t have the agency to start a certain way.\n",
    "With Yes Days, children are told, “okay, what do you want to do?” They get to start themselves! Today, I decided to try that with ChatGPT. I will be asking Chat to just start, and hopefully it comes up with something. I am interested in seeing where the LLM takes the conversation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f34b0f8-bf73-4911-854c-19e36af998d9",
   "metadata": {},
   "source": [
    "### Experiment:\n",
    "\n",
    "<img src=\"GPTNoPlan.png\" width=\"50%\"/>\n",
    "\n",
    "What I think is most interesting about this response is that it reflects more strongly on why ChatGPT thinks I’m here, rather than on a random idea as I was suggesting it do. It almost feels like the model is treating this open-ended question as data as well. When I gave this prompt, I expected it to reply with one of the ideas I had previously been talking about with it. The model mentions finance, anthropology, AI, music, food, and emails as topics I’ve spoken to it about in the past, and I expected it to choose one and either teach me something or go more in depth, maybe ask questions where I would be actively responding and going back and forth.\n",
    "\n",
    "Instead, it seemed to conduct a large brain dump about how I’ve been using this model and what it “likes.”\n",
    "It also talks about hope, which I think is interesting because it speaks about what it hopes that I do, rather than some sort of internal hope of itself. It also says that it likes conversations where it is testing thinking out loud. I find this particularly interesting because, from everything we know about LLMs, it really just tries to string together the most likely next word. When we talk about thinking out loud or thinking deeper rather than just spitting out facts, that’s something a human would be likely to say as well. However, because the LLM is not human, it becomes an almost scary statement. It reads as if the LLM has a mind of its own.\n",
    "\n",
    "I did notice, however, that some of the bullet points it lists aren’t strong examples of what it’s trying to say. For example, it says that I’m almost always asking how something works versus how it’s supposed to work in theory. It also talks about looking at slides instead of readings, or how tone shifts meaning rather than just grammar, which didn’t make much sense to me.\n",
    "I had to push a little further, asking what the LLM meant by its specific example of looking at slides instead of just readings and its answer once again didn’t make much sense:\n",
    "\n",
    "**Me:**\n",
    "> what do you mean by \"power showing up in slides rather than just readings\"\n",
    "**ChatGPT** (excerpt):\n",
    "> **1. Slides are curated, readings are argued.**\n",
    "> A reading usually has to justify itself — it explains assumptions, cites evidence, anticipates objections. Slides don’t. Slides decide *what counts as important* by what they include, exclude, bold, or put first. That act of selection is power.\n",
    "This is particularly interesting as ChatGPT seems to think of the hypothetical concept of slides (i.e. the .pptx files i upload when studying) as entities of their own. When I have directly given Chat the slides in the past before, the LLM takes account of the author. This would be an interesting connection to study in the future.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bc256a-ee6b-4483-b373-faf7af4e002d",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Overall, I found that while ChatGPT did start off by taking a particular direction, it ended up using this non-prompt as a data entry in itself, as it attempted to deduce my intention behind answering the question and based its response off of things that we've spoken about in the past. Notably, it tried to find connections that weren't actually there. One example of this was in how it attempted to maintain the structure of its output rather than prioritizing its logic, since I hadn't given it anything to guide itself on. I think that this is quite telling of the way that the LLM works and this exercise provides a good direction for future prompting. I see that in this scenario, ChatGPT is able to link together the words to perform a visually correct output, but it is the prompt that gives it direction and human reasoning.\n",
    "img\n"
   ]
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
